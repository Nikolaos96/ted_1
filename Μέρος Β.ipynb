{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αικατερίνη Δρακάκη <br>\n",
    "1115201300041 <br>\n",
    "Νικόλαος Μακρυγεώργος <br>\n",
    "1115201500238"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "*****\n",
    "# Vectorization - εξαγωγή χαρακτηριστικών\n",
    "# Προσθήκη Χαρακτηριστικών\n",
    "# SVM  -  KNN   -  ταξινομητές\n",
    "*****\n",
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from collections import Counter\n",
    "#nltk.download('all')\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import random as random\n",
    "import combi\n",
    "from combi import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec \n",
    "from gensim.test.utils import common_texts, get_tmpfile\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn import metrics\n",
    "from nltk import sent_tokenize\n",
    "from random import randint\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import StemmerI, RegexpStemmer, LancasterStemmer, ISRIStemmer, PorterStemmer, SnowballStemmer, RSLPStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φτιαχνουμε λίστες με τα train tweets και τα αποτελέσματα τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neutral'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_character_tweets = \"\"\n",
    "results = []\n",
    "stop__words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"train2017.tsv\", encoding=\"utf8\" ) as file_in_put:\n",
    "    for line in file_in_put:\n",
    "        field = line.split()\n",
    "        results.append(field[2]);\n",
    "        line = line.lower()\n",
    "        list = []\n",
    "        list = line.split()\n",
    "        for i in range(len(list)):\n",
    "            if i > 2 and list[i] not in stop__words and len(list[i]) > 1:\n",
    "                small_character_tweets+=list[i]     \n",
    "                small_character_tweets+=\" \"                \n",
    "        small_character_tweets+=\"\\n\"\n",
    "    del list[:]\n",
    "\n",
    "results.pop(25108)\n",
    "#print (len(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φτιάχνουμε μια λίστα με τα σωστά αποτελέσματα απο τα test δεδομένα για να τα συγκρίνουμε με τα αποτελέσματα των SVM - KNN <br>\n",
    "και να βγάλουμε στατιστικά"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "\n",
    "with open(\"SemEval2017_task4_subtaskA_test_english_gold.txt\", encoding=\"utf8\" ) as test_results__:\n",
    "    for line in test_results__:\n",
    "        field = line.split()\n",
    "        test_results.append(field[1])\n",
    "\n",
    "#print (len(test_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φτιάχνουμε δυο λίστες με τα test δεδομένα.Η λίστα test_tweetList περιέχει κάθε tweet ενω η λίστα tokenized_test_tweets περιέχει <br>\n",
    "λίστες με τις λέξεις κάθε tweet.\n",
    "\n",
    "Επίσης κάνουμε ένα μικρό καθαρισμό των tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_tweets = \"\"\n",
    "with open(\"test2017.tsv\", encoding=\"utf8\" ) as test_tweets__:\n",
    "    for line in test_tweets__:\n",
    "        field = line.split()\n",
    "        line = line.lower()\n",
    "        list = []\n",
    "        list = line.split()\n",
    "        for i in range(len(list)):\n",
    "            if i > 2 and list[i] not in stop__words and len(list[i]) > 1:\n",
    "                str_tweets+=list[i]\n",
    "                str_tweets+=\" \"\n",
    "        str_tweets+=\"\\n\"\n",
    "        del list[:]\n",
    "        \n",
    "str_tweets = str_tweets.replace('#','').replace('@','').replace('-',' ').replace('@','').replace('!','').replace('?','')\n",
    "str_tweets = str_tweets.replace(',',' ').replace('.','').replace('*',' ').replace(':',' ').replace('(',' ').replace(')',' ')\n",
    "\n",
    "test_tweetList = []\n",
    "test_tweetList = str_tweets.split(\"\\n\")\n",
    "test_tweetList.pop(12284)\n",
    "#print (len(test_tweetList))\n",
    "\n",
    "tokenized_test_tweets = []\n",
    "for i in test_tweetList:\n",
    "    f = i.split()\n",
    "    tokenized_test_tweets.append(f)\n",
    "\n",
    "#print (len(tokenized_test_tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Παρακάτω κάνουμε τις ίδιες διαδικασίες για τα test tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_character_tweets = small_character_tweets.replace(',','').replace('-','').replace('_','').replace('<','')\n",
    "small_character_tweets = small_character_tweets.replace('>','').replace('\"  \"\" \"', '').replace('#','')\n",
    "small_character_tweets = small_character_tweets.replace('&','').replace('+','').replace('=','').replace('/','')\n",
    "small_character_tweets = small_character_tweets.replace('(','').replace(')','').replace(':','').replace('!','')\n",
    "small_character_tweets = small_character_tweets.replace('?','').replace(';','').replace('$','')\n",
    "small_character_tweets = small_character_tweets.replace(\"'m\",\" am\").replace('.','')\n",
    "small_character_tweets = small_character_tweets.replace('\"\\\"','').replace('\\\\','').replace('\\'s','').replace('\\'','')\n",
    "\n",
    "small_character_tweets = small_character_tweets.replace('@','').replace('u2019',' ').replace('u002c','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets2 = \"\"\n",
    "tweets2 = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9_]+)', r' user ', small_character_tweets)\n",
    "tweets2 = ' '.join(word for word in tweets2.split(' ') if not word.startswith('http'))\n",
    "\n",
    "tweetList = []\n",
    "tweetList = tweets2.split('\\n')     \n",
    "tweetList.pop(len(tweetList)-1)\n",
    "tweetList.pop(25108)\n",
    "#print (len(tweetList))\n",
    "\n",
    "tokenized_tweets = []\n",
    "for i in tweetList:\n",
    "    f = i.split()\n",
    "    tokenized_tweets.append(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********\n",
    "            Bag of Words\n",
    "            Πρώτα για τα train και μετα για τα test.\n",
    "***********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_Bag = CountVectorizer(max_features=1000)\n",
    "X1 = vectorizer_Bag.fit_transform(tweetList)\n",
    "#print(vectorizer_Bag.get_feature_names())\n",
    "#print(X1.toarray())\n",
    "#print (X1.shape)\n",
    "tweet_list_vector_BGW = X1.toarray()\n",
    "#print (len(tweet_list_vector_BGW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer_Bag_test = CountVectorizer(max_features=1000)\n",
    "X11 = vectorizer_Bag_test.fit_transform(test_tweetList)\n",
    "\n",
    "#print (X11.shape)\n",
    "tweet_list_vector_BGW_test = X11.toarray()\n",
    "#print (len(tweet_list_vector_BGW_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**********\n",
    "             Tf idf\n",
    "             Πρώτα για τα train και μετα για τα test.\n",
    "**********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_TF = TfidfVectorizer(max_features=1000)\n",
    "X2 = vectorizer_TF.fit_transform(tweetList)\n",
    "#print(vectorizer_TF.get_feature_names())\n",
    "#print(X2.shape)\n",
    "#print(X2.toarray())\n",
    "tweet_list_vector_TFIDF = X2.toarray()\n",
    "#print (len(tweet_list_vector_TFIDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_TF_test = TfidfVectorizer(max_features=1000)\n",
    "X22 = vectorizer_TF_test.fit_transform(test_tweetList)\n",
    "\n",
    "tweet_list_vector_TFIDF_test = X22.toarray()\n",
    "#print (len(tweet_list_vector_TFIDF_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****\n",
    "             Word Embeddings\n",
    "             Πρώτα για τα train και μετα για τα test.\n",
    "*****\n",
    "Παρακάτω η μέθοδος Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_w2v = gensim.models.Word2Vec(tokenized_tweets,size=200,window=5,min_count=2)\n",
    "#model_w2v.train(tokenized_tweets, total_examples= len(tokenized_tweets), epochs=20)   \n",
    "#model_w2v.save(\"apotelesmata_train_gia1000.bin\")\n",
    "#model_w2v = Word2Vec.load(\"apotelesmata_train_gia1000.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model_w2v = gensim.models.Word2Vec(tokenized_test_tweets,size=200,window=5,min_count=2)\n",
    "#test_model_w2v.save(\"apotelesmata_test_gia1000.bin\")\n",
    "#test_model_w2v = Word2Vec.load(\"apotelesmata_test_gia1000.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Επειδή δεν έιχαμε καλά αποτελέσματα με τα διανύσματα απο το Work2vec χρησιμοποιούμε τα έτοιμα διανύσματα που δόθηκαν. <br>\n",
    "Αρα διαβάζουμε απο το αρχείο και φτιάχνουμε δυο λίστες,η μία με τις λέξεις και η άλλη με το διάνυσμα καθε λέξης."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordss = []\n",
    "vectors = []\n",
    "\n",
    "with open(\"datastories.twitter.200d.txt\", encoding=\"utf8\" ) as words:\n",
    "    for line in words:\n",
    "        A = line.split()\n",
    "        wordss.append(A[0])\n",
    "        A.pop(0)\n",
    "        vectors.append(A)\n",
    "     \n",
    "#print (len(wordss))\n",
    "#print (len(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Η συνάρτηση mean2 φτιάχνει για κάθε tweet ένα διάνυσμα.Το διάνυσμα κάθε λέξης είναι αυτό απο το αρχείο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean2(tweetList,words,vectors):\n",
    "    meanList_id = []\n",
    "    meanList_vec = []\n",
    "    id = 1\n",
    "    for t in tweetList:\n",
    "        meanList_id.append(id)\n",
    "        id+=1\n",
    "        vecList = []\n",
    "        sum = 0\n",
    "        for j in range(0,200):\n",
    "            vecList.append(0)\n",
    "            \n",
    "            \n",
    "            \n",
    "        tokens = t.split()   \n",
    "        for i in range(len(tokens)):\n",
    "            sum+=1\n",
    "            if tokens[i] in words:\n",
    "                \n",
    "                for k in range(len(words)):\n",
    "                    if tokens[i] == words[k]:\n",
    "                        vec = vectors[k]\n",
    "                        for j in range(0,200):\n",
    "                            vecList[j] = vecList[j] + float(vec[j])\n",
    "                        break\n",
    "                        \n",
    "            else:\n",
    "                for j in range(0,200):\n",
    "                    vecList[j] = vecList[j] + random.uniform(-1, 1)\n",
    "        \n",
    "        \n",
    "        for j in range(0,200):\n",
    "            vecList[j] = vecList[j]/sum\n",
    "        meanList_vec.append(vecList)\n",
    "        \n",
    "    return meanList_id, meanList_vec\n",
    "\n",
    "\n",
    "\n",
    "tweet_list_id, tweet_list_vector_WEM = mean2(tweetList,wordss,vectors)\n",
    "test_tweet_id, test_tweet_WEM = mean2(test_tweetList,wordss,vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ιδια διαδικασία με την παραπάνω αλλά για την μέθοδο Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mean(tweetList,model_w2v):\n",
    "    wordModel = model_w2v.wv\n",
    "    meanList_id = []\n",
    "    meanList_vec = []\n",
    "    id = 1\n",
    "    for t in tweetList:\n",
    "        meanList_id.append(id)\n",
    "        id+=1\n",
    "        vecList = []\n",
    "        sum = 0\n",
    "        for j in range(0,200):\n",
    "            vecList.append(0)\n",
    "    \n",
    "    \n",
    "    \n",
    "        tokens = t.split()\n",
    "        for i in range(len(tokens)):\n",
    "            sum+=1\n",
    "            if tokens[i] in wordModel.vocab:\n",
    "                vector = model_w2v[tokens[i]]\n",
    "                for j in range(0,200):\n",
    "                    vecList[j] = vecList[j] + vector[j]\n",
    "            else:\n",
    "                for j in range(0,200):\n",
    "                    vecList[j] = vecList[j] + random.uniform(-1, 1)\n",
    "    \n",
    "    \n",
    "        for j in range(0,200):\n",
    "            vecList[j] = vecList[j]/sum\n",
    "        meanList_vec.append(vecList)\n",
    "\n",
    "        \n",
    "    return meanList_id, meanList_vec\n",
    "\n",
    "#tweet_list_id, tweet_list_vector_WEM = mean(tweetList,model_w2v)\n",
    "#test_tweet_id, test_tweet_WEM = mean(test_tweetList,test_model_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Αντικαθιστούμε τα συναισθήματα με 0,1,2\n",
    "Δουλεύει και χωρίς αυτό."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative = 0\n",
    "# neutral = 1\n",
    "# positive = 2\n",
    "\n",
    "#print (results)\n",
    "#for i in range(len(results)):\n",
    "#    if results[i] == \"negative\":\n",
    "#        results[i] = 0\n",
    "#    elif results[i] == \"neutral\":\n",
    "#        results[i] = 1\n",
    "#    else:\n",
    "#        results[i] = 2\n",
    "#print (results)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(test_results)):\n",
    "#    if test_results[i] == \"negative\":\n",
    "#        test_results[i] = 0\n",
    "#    elif test_results[i] == \"neutral\":\n",
    "#        test_results[i] = 1\n",
    "#    else:\n",
    "#        test_results[i] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "     Ταξινομητής   SVM \n",
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3697537528373704"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM - Bag of Words\n",
    "\n",
    "svc_BOW = svm.SVC(kernel='linear', C=1, probability=True)  \n",
    "svc_BOW = svc_BOW.fit(tweet_list_vector_BGW , results)\n",
    "results_BOW = svc_BOW.predict(tweet_list_vector_BGW_test) \n",
    "f1_score(test_results, results_BOW, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33729129033705113"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM - Tf Idf\n",
    "\n",
    "svc_TfIdf = svm.SVC(kernel='linear', C=1, probability=True)\n",
    "svc_TfIdf = svc_TfIdf.fit(tweet_list_vector_TFIDF , results)\n",
    "results_TfIdf = svc_TfIdf.predict(tweet_list_vector_TFIDF_test)\n",
    "f1_score(test_results, results_TfIdf, average='macro') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5670108940179143"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM - Word Embeddings\n",
    "\n",
    "svc_WEMBEDDINGS = svm.SVC(kernel='linear', C=1, probability=True) \n",
    "svc_WEMBEDDINGS = svc_WEMBEDDINGS.fit(tweet_list_vector_WEM, results)\n",
    "results_WEMBEDDINGS = svc_WEMBEDDINGS.predict(test_tweet_WEM)\n",
    "f1_score(test_results, results_WEMBEDDINGS, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "     Ταξινομητής   ΚΝΝ\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4832302181699772\n"
     ]
    }
   ],
   "source": [
    "# KNN - Bag of Words\n",
    "\n",
    "knn_BOW = KNeighborsClassifier(n_neighbors=100)\n",
    "knn_BOW.fit(tweet_list_vector_BGW , results)\n",
    "results_BOW = knn_BOW.predict(tweet_list_vector_BGW_test)\n",
    "print(metrics.accuracy_score(test_results, results_BOW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4826603712145881\n"
     ]
    }
   ],
   "source": [
    "# KNN - Tf Idf \n",
    "\n",
    "knn_TfIdf = KNeighborsClassifier(n_neighbors=100) \n",
    "knn_TfIdf.fit(tweet_list_vector_TFIDF , results)\n",
    "results_TfIdf = knn_TfIdf.predict(tweet_list_vector_TFIDF_test)\n",
    "print(metrics.accuracy_score(test_results, results_TfIdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.572614783458157\n"
     ]
    }
   ],
   "source": [
    "# KNN - Word Embeddings\n",
    "\n",
    "knn_WEMBEDDINGS = KNeighborsClassifier(n_neighbors=150)\n",
    "knn_WEMBEDDINGS.fit(tweet_list_vector_WEM , results)\n",
    "results_WEMBEDDINGS = knn_WEMBEDDINGS.predict(test_tweet_WEM)\n",
    "print(metrics.accuracy_score(test_results, results_WEMBEDDINGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "****\n",
    "****\n",
    "Παρακάτω κάνουμε ΚΝΝ και SVM με προσθήκη χαρακτηριστικών.\n",
    "\n",
    "Βάζουμε σε λίστες κάθε λεξικό μαζί με το συναίσθημα του."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex1 = []\n",
    "lex2 = []\n",
    "lex3 = []\n",
    "lex4 = []\n",
    "lex5 = []\n",
    "\n",
    "def lexica_in_list(file):\n",
    "    lexica = []\n",
    "    with open(file, encoding=\"utf8\") as f:\n",
    "        for lines in f:\n",
    "            k = lines.split()\n",
    "            length = len(k)\n",
    "            str = \"\"\n",
    "            for i in range(len(k) -1):\n",
    "                if i > 0:\n",
    "                    str+= \" \"\n",
    "                str+= k[i]\n",
    "            lexica.append(str)\n",
    "            lexica.append(k[length-1])\n",
    "            \n",
    "    return lexica\n",
    "\n",
    "lex1 = lexica_in_list(\"affin.txt\")\n",
    "lex2 = lexica_in_list(\"valence_tweet.txt\")\n",
    "lex3 = lexica_in_list(\"generic.txt\")\n",
    "lex4 = lexica_in_list(\"nrc.txt\")\n",
    "lex5 = lexica_in_list(\"nrctag.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Υπολογίζουμε τα νέα χαρακτηριστικά για κάθε tweet.Αυτά είναι : <br>\n",
    "το μικρότερο συναίσθημα του tweet <br>\n",
    "το μεγαλύτερο συναίσθημα του tweet <br>\n",
    "ο μέσος όρος συναισθημάτων <br>\n",
    "Αυτη η διαδικασία γίνεται για κάθε λεξικό. <br>\n",
    "Αν κάποια λέξη δεν υπάρχει στο λεξικό τότε παίρνει τυχαία τιμή συναισθήματος. <br> \n",
    "<br>\n",
    "H παρακάτω διαδικασία είναι πολύ χρονοβόρα για το λόγο αυτό τα αποθηκεύουμε και τα κάνουμε την επόμενη φορά load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = []\n",
    "l2 = []\n",
    "l3 = []\n",
    "l4 = []\n",
    "l5 = []\n",
    "\n",
    "def new_features(tweetList, lex, kk):\n",
    "    l2 = []\n",
    "    for tweet in tweetList:\n",
    "        l = []\n",
    "        t = tweet.split()\n",
    "        min = 1000\n",
    "        max = -1000\n",
    "        sum = 0\n",
    "        for i in range(len(t)):\n",
    "            find= 0\n",
    "            for j in range(len(lex))[::2]:\n",
    "                if t[i] == lex[j]:\n",
    "                    find = 1\n",
    "                    if float(lex[j+1]) < min:\n",
    "                        min = float(lex[j+1])\n",
    "                    if float(lex[j+1]) > max:\n",
    "                        max = float(lex[j+1])\n",
    "                    sum+=float(lex[j+1])\n",
    "                    break\n",
    "            if find == 0:\n",
    "                if kk == 1:\n",
    "                    word = randint(-3,3)\n",
    "                elif kk == 2:\n",
    "                    word = random.uniform(-1, 1)\n",
    "                elif kk == 3:\n",
    "                    word = random.uniform(-0.9, 0.9)\n",
    "                else:\n",
    "                    word = random.uniform(-5, 5)\n",
    "                if word < min:\n",
    "                    min = word\n",
    "                if word > max:\n",
    "                    max = word\n",
    "                sum+=word\n",
    "                    \n",
    "        sum = sum/len(t)\n",
    "        l.append(min)\n",
    "        l.append(max)\n",
    "        l.append(sum)\n",
    "        l2.append(l)\n",
    "\n",
    "    return l2\n",
    "    \n",
    "#l1 = new_features(tweetList, lex1, 1)\n",
    "#l2 = new_features(tweetList, lex2, 2)\n",
    "#l3 = new_features(tweetList, lex3, 3)\n",
    "#l4 = new_features(tweetList, lex4, 4)\n",
    "#l5 = new_features(tweetList, lex5, 5)\n",
    "\n",
    "#with open(\"l1.txt\", \"wb\") as fp1:\n",
    "    #pickle.dump(l1, fp1)\n",
    "#with open(\"l2.txt\", \"wb\") as fp2:\n",
    "    #pickle.dump(l2, fp2)\n",
    "#with open(\"l3.txt\", \"wb\") as fp3:\n",
    " #   pickle.dump(l3, fp3)\n",
    "#with open(\"l4.txt\", \"wb\") as fp4:\n",
    "    #pickle.dump(l4, fp4)\n",
    "#with open(\"l5.txt\", \"wb\") as fp5:\n",
    "    #pickle.dump(l5, fp5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φόρτωση χαρακτηριστηκών για κάθε tweet και κάθε λεξικό σε λίστες."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"l1.txt\", \"rb\") as fp1:\n",
    "    l1 = pickle.load(fp1) \n",
    "with open(\"l2.txt\", \"rb\") as fp2:\n",
    "    l2 = pickle.load(fp2)\n",
    "with open(\"l3.txt\", \"rb\") as fp3:\n",
    "    l3 = pickle.load(fp3)\n",
    "with open(\"l4.txt\", \"rb\") as fp4:\n",
    "    l4 = pickle.load(fp4)\n",
    "with open(\"l5.txt\", \"rb\") as fp5:\n",
    "    l5 = pickle.load(fp5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Δημιουργία χαρακτηριστικών για τα test δεδομένα"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#l1_test = new_features(test_tweetList, lex1, 1)\n",
    "#l2_test = new_features(test_tweetList, lex2, 2)\n",
    "#l3_test = new_features(test_tweetList, lex3, 3)\n",
    "#l4_test = new_features(test_tweetList, lex4, 4)\n",
    "#l5_test = new_features(test_tweetList, lex5, 5)\n",
    "\n",
    "#with open(\"l1_test.txt\", \"wb\") as t1:\n",
    "    #pickle.dump(l1_test, t1)\n",
    "#with open(\"l2_test.txt\", \"wb\") as t2:\n",
    "  #  pickle.dump(l2_test, t2)\n",
    "#with open(\"l3_test.txt\", \"wb\") as t3:\n",
    "  #  pickle.dump(l3_test, t3)\n",
    "#with open(\"l4_test.txt\", \"wb\") as t4:\n",
    "    #pickle.dump(l4_test, t4)\n",
    "#with open(\"l5_test.txt\", \"wb\") as t5:\n",
    "    #pickle.dump(l5_test, t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Φόρτωση χαρακτηριστηκών για κάθε tweet και κάθε λεξικό σε λίστες για τα test δεδομένα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"l1_test.txt\", \"rb\") as t1:\n",
    "    l1_test = pickle.load(t1)\n",
    "with open(\"l2_test.txt\", \"rb\") as t2:\n",
    "    l2_test = pickle.load(t2)\n",
    "with open(\"l3_test.txt\", \"rb\") as t3:\n",
    "    l3_test = pickle.load(t3)\n",
    "with open(\"l4_test.txt\", \"rb\") as t4:\n",
    "    l4_test = pickle.load(t4)\n",
    "with open(\"l5_test.txt\", \"rb\") as t5:\n",
    "    l5_test = pickle.load(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Τα νέα χαρακτηριστικά τα προσθέτουμε στα διανύσματα που ήδη υπάρχουν."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweet_list_vector_WEM)):\n",
    "    tweet_list_vector_WEM[i].append(l1[i][0])\n",
    "    tweet_list_vector_WEM[i].append(l1[i][1])\n",
    "    tweet_list_vector_WEM[i].append(l1[i][2])\n",
    "    \n",
    "    tweet_list_vector_WEM[i].append(l2[i][0])\n",
    "    tweet_list_vector_WEM[i].append(l2[i][1])\n",
    "    tweet_list_vector_WEM[i].append(l2[i][2])\n",
    "    \n",
    "    tweet_list_vector_WEM[i].append(l3[i][0])\n",
    "    tweet_list_vector_WEM[i].append(l3[i][1])\n",
    "    tweet_list_vector_WEM[i].append(l3[i][2])\n",
    "    \n",
    "    tweet_list_vector_WEM[i].append(l4[i][0])\n",
    "    tweet_list_vector_WEM[i].append(l4[i][1])\n",
    "    tweet_list_vector_WEM[i].append(l4[i][2])\n",
    "    \n",
    "    tweet_list_vector_WEM[i].append(l5[i][0])\n",
    "    tweet_list_vector_WEM[i].append(l5[i][1])\n",
    "    tweet_list_vector_WEM[i].append(l5[i][2])\n",
    "    \n",
    "\n",
    "for i in range(len(test_tweet_WEM)):\n",
    "    test_tweet_WEM[i].append(l1_test[i][0])\n",
    "    test_tweet_WEM[i].append(l1_test[i][1])\n",
    "    test_tweet_WEM[i].append(l1_test[i][2])\n",
    "    \n",
    "    test_tweet_WEM[i].append(l2_test[i][0])\n",
    "    test_tweet_WEM[i].append(l2_test[i][1])\n",
    "    test_tweet_WEM[i].append(l2_test[i][2])\n",
    "    \n",
    "    test_tweet_WEM[i].append(l3_test[i][0])\n",
    "    test_tweet_WEM[i].append(l3_test[i][1])\n",
    "    test_tweet_WEM[i].append(l3_test[i][2])\n",
    "    \n",
    "    test_tweet_WEM[i].append(l4_test[i][0])\n",
    "    test_tweet_WEM[i].append(l4_test[i][1])\n",
    "    test_tweet_WEM[i].append(l4_test[i][2])\n",
    "    \n",
    "    test_tweet_WEM[i].append(l5_test[i][0])\n",
    "    test_tweet_WEM[i].append(l5_test[i][1])\n",
    "    test_tweet_WEM[i].append(l5_test[i][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    " SVM + KNN  για Word Embeddings + new features\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.583986854473807"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM - Word Embeddings + new features\n",
    "\n",
    "svc_WEMBEDDINGS = svm.SVC(kernel='linear', C=1, probability=True) \n",
    "svc_WEMBEDDINGS = svc_WEMBEDDINGS.fit(tweet_list_vector_WEM, results)\n",
    "results_WEMBEDDINGS = svc_WEMBEDDINGS.predict(test_tweet_WEM)\n",
    "f1_score(test_results, results_WEMBEDDINGS, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Με αυτα τα 15 χαρακτηριστικά που προσθέσαμε βλέπουμε μια μικρή αύξηση απο 0.56 σε 0.58\n",
    "****\n",
    "Για τον ΚΝΝ θα αφήσουμε μόνο τον μέσο ορο δηλαδή θα έχει μόνο 5 επιπλέον χαρακτηριστικά. <br>\n",
    "Με τα 15 επιπλέον όπως ο SVM είχε αρνητικά αποτελέσματα ετσι τα αφαιρούμε τα 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(tweet_list_vector_WEM)):\n",
    "    tweet_list_vector_WEM[j].pop(200)\n",
    "    tweet_list_vector_WEM[j].pop(200)  \n",
    "    tweet_list_vector_WEM[j].pop(201)\n",
    "    tweet_list_vector_WEM[j].pop(201)\n",
    "    tweet_list_vector_WEM[j].pop(202)\n",
    "    tweet_list_vector_WEM[j].pop(202)\n",
    "    tweet_list_vector_WEM[j].pop(203)\n",
    "    tweet_list_vector_WEM[j].pop(203)\n",
    "    tweet_list_vector_WEM[j].pop(204)\n",
    "    tweet_list_vector_WEM[j].pop(204)\n",
    "    \n",
    "for j in range(len(test_tweet_WEM)):\n",
    "    test_tweet_WEM[j].pop(200)\n",
    "    test_tweet_WEM[j].pop(200)\n",
    "    test_tweet_WEM[j].pop(201)\n",
    "    test_tweet_WEM[j].pop(201)\n",
    "    test_tweet_WEM[j].pop(202)\n",
    "    test_tweet_WEM[j].pop(202)\n",
    "    test_tweet_WEM[j].pop(203)\n",
    "    test_tweet_WEM[j].pop(203)\n",
    "    test_tweet_WEM[j].pop(204)\n",
    "    test_tweet_WEM[j].pop(204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5653695864539238\n"
     ]
    }
   ],
   "source": [
    "# KNN - Word Embeddings + new features\n",
    "\n",
    "knn_WEMBEDDINGS = KNeighborsClassifier(n_neighbors=150)\n",
    "knn_WEMBEDDINGS.fit(tweet_list_vector_WEM , results)\n",
    "results_WEMBEDDINGS = knn_WEMBEDDINGS.predict(test_tweet_WEM)\n",
    "print(metrics.accuracy_score(test_results, results_WEMBEDDINGS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
